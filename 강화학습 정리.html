<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>강화학습의 핵심 개념: 이론적 배경부터 실제 적용까지</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <style>
        body {
            font-family: 'Malgun Gothic', 'Nanum Gothic', sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 {
            color: #1a73e8;
            text-align: center;
            font-size: 32px;
            margin-top: 40px;
        }
        h2 {
            color: #1a73e8;
            border-bottom: 2px solid #1a73e8;
            padding-bottom: 5px;
            margin-top: 30px;
        }
        h3 {
            color: #185abc;
            margin-top: 25px;
        }
        ul, ol {
            padding-left: 25px;
        }
        li {
            margin-bottom: 10px;
        }
        hr {
            height: 3px;
            background-color: #dadce0;
            border: none;
            margin: 40px 0;
        }
        .code {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
            margin: 15px 0;
        }
        strong {
            color: #185abc;
        }
        .slide {
            page-break-after: always;
            margin-bottom: 50px;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
        }
        .formula {
            text-align: center;
            margin: 20px 0;
            font-style: italic;
        }
        .footnote {
            font-size: 14px;
            color: #666;
            margin-top: 40px;
            border-top: 1px solid #ddd;
            padding-top: 10px;
        }
    </style>
</head>
<body>

<div class="slide">
    <h1>강화학습의 핵심 개념</h1>
    <h2>이론적 배경부터 실제 적용까지</h2>
</div>

<hr>

<div class="slide">
    <h2>목차</h2>
    <ol>
        <li>강화학습 환경과 MDP 프레임워크</li>
        <li>태스크 분류와 특성</li>
        <li>보상과 가치 함수의 구분과 역할</li>
        <li>가치 함수 갱신과 내분점 개념</li>
        <li>정책(Policy) 접근법과 표현 방식</li>
        <li>Monte Carlo 방법의 이론적 배경</li>
        <li>Q 학습 알고리즘</li>
        <li>벨만 방정식과 학습 접근법</li>
    </ol>
</div>

<hr>

<div class="slide">
    <h2>1. 강화학습 환경과 MDP 프레임워크</h2>
    <h3>강화학습의 기본 구성요소</h3>
    <ul>
        <li><strong>에이전트(Agent)</strong>: 학습하고 결정을 내리는 주체</li>
        <li><strong>환경(Environment)</strong>: 에이전트가 상호작용하는 세계</li>
        <li><strong>상태(State) s</strong>: 환경의 현재 상황</li>
        <li><strong>행동(Action) a</strong>: 에이전트의 선택</li>
        <li><strong>보상(Reward) r</strong>: 행동의 결과로 받는 피드백</li>
        <li><strong>다음 상태(Next State) s'</strong>: 행동 후 변화된 환경</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>1. 강화학습 환경과 MDP 프레임워크 (계속)</h2>
    <h3>마르코프 결정 프로세스(MDP)</h3>
    <ul>
        <li><strong>마르코프 속성</strong>: 미래는 현재 상태에만 의존
            <div class="formula">
                P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1},...) = P(s_{t+1}|s_t, a_t)
            </div>
        </li>
        <li><strong>MDP 정의</strong>: 튜플 (S, A, P, R, γ)
            <ul>
                <li>S: 상태 집합</li>
                <li>A: 행동 집합</li>
                <li>P: 상태 전이 확률 함수 P(s'|s,a)</li>
                <li>R: 보상 함수 R(s,a,s')</li>
                <li>γ: 할인 인자 (0 ≤ γ ≤ 1)</li>
            </ul>
        </li>
        <li><strong>목표</strong>: 기대 누적 보상을 최대화하는 정책 π 찾기</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>2. 태스크(Task) 분류와 특성</h2>
    <h3>에피소드 태스크(Episodic Task)</h3>
    <ul>
        <li><strong>특징</strong>: 명확한 시작과 종료 상태 존재</li>
        <li><strong>예시</strong>: 미로 탈출, 체스 한 판 이기기</li>
        <li><strong>가치 계산</strong>: \(G_t = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k+1}\)</li>
    </ul>

    <h3>연속 태스크(Continuing Task)</h3>
    <ul>
        <li><strong>특징</strong>: 종료 없이 계속 진행</li>
        <li><strong>예시</strong>: 로봇 균형 유지, 온도 조절</li>
        <li><strong>가치 계산</strong>: \(G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}\) (γ < 1 필요)</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>3. 보상과 가치 함수의 구분과 역할</h2>
    <h3>보상(Reward)과 가치함수의 관계</h3>
    <ul>
        <li><strong>보상(r)</strong>: 한 단계에서 얻는 즉각적인 피드백</li>
        <li><strong>가치함수</strong>: 장기적인 누적 보상의 기대값</li>
        <li><strong>관계</strong>: 가치함수는 미래 보상들의 할인된 합의 예측값</li>
    </ul>

    <h3>상태 가치 함수 V(s)</h3>
    <ul>
        <li><strong>정의</strong>: 상태 s에서 정책 π를 따를 때 기대되는 미래 보상 합</li>
        <li><strong>수식</strong>: \(V^\pi(s) = E_\pi[G_t|S_t=s]\)</li>
        <li><strong>해석</strong>: "이 상태에 있으면 앞으로 얼마나 좋을까?"</li>
    </ul>

    <h3>행동 가치 함수 Q(s,a)</h3>
    <ul>
        <li><strong>정의</strong>: 상태 s에서 행동 a 후 정책 π를 따를 때 기대되는 미래 보상 합</li>
        <li><strong>수식</strong>: \(Q^\pi(s,a) = E_\pi[G_t|S_t=s, A_t=a]\)</li>
        <li><strong>해석</strong>: "이 상태에서 이 행동을 하면 앞으로 얼마나 좋을까?"</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>3. 보상과 가치 함수의 구분과 역할 (계속)</h2>
    <h3>가치함수의 맥락적 의미</h3>
    <ul>
        <li><strong>단기적 vs 장기적 가치</strong>: 보상(r)은 즉각적 피드백, 가치함수는 장기적 결과 예측</li>
        <li><strong>탐색-활용 균형</strong>: 가치함수는 미래 가능성을 평가하여 탐색과 활용 사이의 의사결정 지원</li>
        <li><strong>학습 진행 척도</strong>: 가치함수의 변화는 에이전트 학습 진행 상황을 나타냄</li>
    </ul>

    <h3>두 함수의 관계</h3>
    <ul>
        <li>\(V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)\)
            <ul>
                <li>"상태의 가치 = 가능한 모든 행동의 가치의 확률 가중 평균"</li>
            </ul>
        </li>
        <li>\(Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')\)
            <ul>
                <li>"행동의 가치 = 즉각적 보상 + 다음 상태들의 할인된 가치의 기대값"</li>
            </ul>
        </li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>4. 가치 함수 갱신과 내분점 개념</h2>
    <h3>Monte Carlo vs Temporal Difference 갱신</h3>

    <h4>Monte Carlo 갱신:</h4>
    <ul>
        <li><strong>특징</strong>: 완전한 에피소드 후 갱신</li>
        <li><strong>목표값</strong>: 실제 관측된 리턴 \(G_t\)</li>
        <li><strong>갱신 공식</strong>: \(V(s) \leftarrow V(s) + \alpha[G_t - V(s)]\)</li>
        <li><strong>내분점 형태</strong>: \(V(s) \leftarrow (1-\alpha)V(s) + \alpha G_t\)</li>
    </ul>

    <h4>Temporal Difference 갱신:</h4>
    <ul>
        <li><strong>특징</strong>: 매 단계 후 즉시 갱신</li>
        <li><strong>목표값</strong>: \(r + \gamma V(s')\)</li>
        <li><strong>갱신 공식</strong>: \(V(s) \leftarrow V(s) + \alpha[r + \gamma V(s') - V(s)]\)</li>
        <li><strong>내분점 형태</strong>: \(V(s) \leftarrow (1-\alpha)V(s) + \alpha[r + \gamma V(s')]\)</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>4. 가치 함수 갱신과 내분점 개념 (계속)</h2>
    <h3>내분점 개념의 직관적 이해</h3>
    <ul>
        <li><strong>기하학적 의미</strong>: 두 점 사이의 위치를 α:(1-α) 비율로 결정</li>
        <li><strong>학습 과정 해석</strong>: "내가 알고 있던 것"과 "새롭게 경험한 것" 사이의 균형</li>
        <li><strong>학습률 α의 역할</strong>: 
            <ul>
                <li>α = 0: 완전히 기존 추정치 유지</li>
                <li>α = 1: 완전히 새 목표값으로 이동</li>
                <li>0 < α < 1: 두 값 사이의 내분점</li>
            </ul>
        </li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>5. 정책(Policy) 접근법과 표현 방식</h2>
    <h3>On-Policy vs Off-Policy 학습</h3>

    <h4>On-Policy 학습:</h4>
    <ul>
        <li>행동 정책과 목표 정책이 <strong>동일</strong></li>
        <li>"내가 행동하는 그 정책을 직접 개선"</li>
        <li>예: SARSA, Actor-Critic</li>
    </ul>

    <h4>Off-Policy 학습:</h4>
    <ul>
        <li>행동 정책과 목표 정책이 <strong>다름</strong></li>
        <li>"한 정책으로 행동하면서 다른 정책을 개선"</li>
        <li>예: Q-learning, DQN</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>5. 정책(Policy) 접근법과 표현 방식 (계속)</h2>
    <h3>정책 표현 방식 비유</h3>

    <h4>행동 정책(Behavior Policy):</h4>
    <ul>
        <li><strong>Label Smoothing과 유사</strong></li>
        <li>ε-greedy 예시:
            <div class="code">
            π_b(a_best|s) = 1-ε<br>
            π_b(기타 행동|s) = ε/(행동 수-1)
            </div>
        </li>
        <li>탐색(exploration)을 위한 "부드러운" 확률 분포</li>
    </ul>

    <h4>목표 정책(Target Policy):</h4>
    <ul>
        <li><strong>One-hot Encoding과 유사</strong></li>
        <li>Greedy 정책 예시:
            <div class="code">
            π_t(a_best|s) = 1.0<br>
            π_t(기타 행동|s) = 0
            </div>
        </li>
        <li>활용(exploitation)에 집중하는 "결정적인" 정책</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>6. Monte Carlo 방법의 이론적 배경</h2>
    <h3>MCMC와 강화학습의 관계</h3>
    <ul>
        <li><strong>MCMC(Markov Chain Monte Carlo)</strong>: 복잡한 확률 분포에서 샘플 추출</li>
        <li><strong>공통점</strong>: 샘플링을 통한 기대값 추정</li>
        <li><strong>이론적 기반</strong>: 대수의 법칙(Law of Large Numbers)</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>6. Monte Carlo 방법의 이론적 배경 (계속)</h2>
    <h3>Monte Carlo 방법의 핵심 원리</h3>
    <ul>
        <li><strong>핵심</strong>: 많은 무작위 샘플로 기대값 추정</li>
        <li><strong>통계적 수렴성</strong>: 실험 횟수가 증가할수록 표본 평균이 실제 모수에 수렴
            <div class="formula">
                \(\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i\)
            </div>
            <div class="formula">
                \(P(|\bar{X}_n - \mu| < \epsilon) \to 1\) (n → ∞)
            </div>
        </li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>6. Monte Carlo 방법의 통계적 기반</h2>
    <h3>대수의 법칙과 통계적 수렴</h3>
    <ul>
        <li><strong>약한 대수의 법칙</strong>: 표본 크기가 증가하면, 표본 평균이 확률적으로 모평균에 수렴
            <div class="formula">
                \(\lim_{n \to \infty} P(|\bar{X}_n - \mu| < \epsilon) = 1\) (∀ ε > 0)
            </div>
        </li>
        <li><strong>강한 대수의 법칙</strong>: 표본 크기가 증가하면, 표본 평균이 거의 확실하게 모평균에 수렴
            <div class="formula">
                \(P(\lim_{n \to \infty} \bar{X}_n = \mu) = 1\)
            </div>
        </li>
        <li><strong>신뢰구간</strong>: 모수가 특정 범위 내에 있을 확률적 보장
            <div class="formula">
                \(P(\mu \in [\bar{X}_n - \delta, \bar{X}_n + \delta]) \approx 1 - \alpha\)
            </div>
            여기서 δ는 오차 범위, α는 유의 수준
        </li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>6. Monte Carlo 방법의 강화학습 적용</h2>
    <h3>가치 함수 추정에서의 Monte Carlo</h3>
    <ul>
        <li><strong>강화학습 적용</strong>: 완전한 에피소드로 가치 함수 추정</li>
        <li><strong>분산 감소 기법</strong>: 
            <ul>
                <li>First-visit MC vs Every-visit MC</li>
                <li>중요도 샘플링(Importance Sampling)</li>
                <li>기준선(Baseline) 사용</li>
            </ul>
        </li>
        <li><strong>구체적 방법</strong>:
            <ol>
                <li>정책 π를 따라 여러 에피소드 생성</li>
                <li>상태 s 방문시 리턴 \(G_t\) 기록</li>
                <li>V(s)를 모든 방문 리턴의 평균으로 추정</li>
            </ol>
        </li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>7. 구체적인 Q 학습 알고리즘</h2>
    <h3>Q-learning (Off-policy TD)</h3>
    <ol>
        <li>모든 Q(s,a) 초기화</li>
        <li>각 에피소드마다:
            <ul>
                <li>초기 상태 s 관찰</li>
                <li>종료될 때까지 반복:
                    <ul>
                        <li>ε-greedy로 행동 a 선택 (행동 정책)</li>
                        <li>행동 a 취하고, 보상 r과 다음 상태 s' 관찰</li>
                        <li>\(Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\)</li>
                        <li>s ← s'</li>
                    </ul>
                </li>
            </ul>
        </li>
    </ol>

    <h4>내분점 관점:</h4>
    <ul>
        <li>현재 Q값과 "즉각적 보상 + 최적 다음 행동의 할인된 Q값" 사이의 내분점</li>
        <li>목표 정책: 완전 greedy(one-hot), 행동 정책: ε-greedy(label smoothing)</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>7. 구체적인 Q 학습 알고리즘 (계속)</h2>
    <h3>SARSA (On-policy TD)</h3>
    <ol>
        <li>모든 Q(s,a) 초기화</li>
        <li>각 에피소드마다:
            <ul>
                <li>초기 상태 s 관찰</li>
                <li>ε-greedy로 행동 a 선택</li>
                <li>종료될 때까지 반복:
                    <ul>
                        <li>행동 a 취하고, 보상 r과 다음 상태 s' 관찰</li>
                        <li>ε-greedy로 다음 행동 a' 선택</li>
                        <li>\(Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]\)</li>
                        <li>s ← s', a ← a'</li>
                    </ul>
                </li>
            </ul>
        </li>
    </ol>

    <h4>내분점 관점:</h4>
    <ul>
        <li>현재 Q값과 "즉각적 보상 + 실제 다음 행동의 할인된 Q값" 사이의 내분점</li>
        <li>행동 정책과 목표 정책이 동일한 ε-greedy</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>8. 벨만 방정식과 가치 기반/정책 기반 접근법</h2>
    <h3>벨만 방정식의 배경과 의미</h3>
    <ul>
        <li><strong>기원</strong>: 리처드 벨만이 1950년대에 개발한 동적 프로그래밍 기법</li>
        <li><strong>최적성의 원리</strong>: 최적 정책의 부분도 최적이어야 함</li>
    </ul>

    <h4>벨만 기대 방정식:</h4>
    <div class="formula">
        \(V^\pi(s) = \sum_a \pi(a|s) [R(s,a) + \gamma \sum_{s'} P(s'|s,a)V^\pi(s')]\)
    </div>
    <p>"현재 가치 = 즉각적 보상 + 할인된 미래 가치"</p>

    <h4>벨만 최적 방정식:</h4>
    <div class="formula">
        \(V^*(s) = \max_a [R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s')]\)
    </div>
    <p>최적 정책 하에서의 가치 함수 조건</p>
</div>

<hr>

<div class="slide">
    <h2>8. 벨만 방정식과 가치 기반/정책 기반 접근법 (계속)</h2>
    <h3>가치 기반 vs 정책 기반 접근법</h3>

    <h4>Value-based 방법:</h4>
    <ul>
        <li>Q함수를 학습하여 간접적으로 정책 도출</li>
        <li>정책 함수: \(\pi(s) = \arg\max_a Q(s,a)\)</li>
        <li>Q 함수는 정책 함수의 "가중치" 역할</li>
        <li>예: Q-learning, DQN</li>
    </ul>

    <h4>Policy-based 방법:</h4>
    <ul>
        <li>정책 함수 \(\pi(a|s;\theta)\)를 직접 파라미터화하고 학습</li>
        <li>목적 함수 \(J(\theta) = E_{\pi_\theta}[\sum_t \gamma^t r_t]\)를 최대화</li>
        <li>그래디언트 기반 업데이트: \(\theta \leftarrow \theta + \alpha\nabla_\theta J(\theta)\)</li>
        <li>예: REINFORCE, PPO</li>
    </ul>
</div>

<hr>

<div class="slide">
    <h2>결론: 강화학습의 통합적 이해</h2>
    <p>강화학습의 핵심 개념들은 유기적으로 연결됩니다:</p>
    <ol>
        <li><strong>환경과 MDP</strong>: 강화학습 문제의 수학적 프레임워크</li>
        <li><strong>태스크 분류</strong>: 에피소드와 연속 태스크의 특성에 따른 접근법</li>
        <li><strong>보상과 가치 함수</strong>: 즉각적 피드백과 장기적 가치 예측</li>
        <li><strong>가치 함수 갱신</strong>: 내분점 개념을 통한 점진적 학습</li>
        <li><strong>정책 접근법</strong>: 가치함수를 바탕으로 한 행동 결정 방식</li>
        <li><strong>Monte Carlo 방법</strong>: 경험 샘플링을 통한 통계적 학습</li>
        <li><strong>Q-learning, SARSA</strong>: 구체적 알고리즘의 작동 원리</li>
        <li><strong>벨만 방정식과 접근법</strong>: 이론적 기반과 학습 방법론</li>
    </ol>
</div>

<hr>

<div class="slide">
    <h1>감사합니다</h1>
    <h3>질문이 있으신가요?</h3>
</div>

</body>
</html>